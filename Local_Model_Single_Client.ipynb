{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "abpOWHPT3hNL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "import base64\n",
        "import sys\n",
        "\n",
        "import hashlib\n",
        "import json\n",
        "from time import time\n",
        "from time import sleep\n",
        "import requests\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [5, 5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MNIST('/kaggle/working', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = MNIST('/kaggle/working', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train_dataset, dev_dataset = random_split(train_dataset, [int(len(train_dataset) * 0.83), int(len(train_dataset) * 0.17)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZnMuR0B3mm3",
        "outputId": "0515fe6e-48a7-4777-a322-1fb7e32f8cf8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /kaggle/working/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [04:35<00:00, 36000.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /kaggle/working/MNIST/raw/train-images-idx3-ubyte.gz to /kaggle/working/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /kaggle/working/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1628299.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /kaggle/working/MNIST/raw/train-labels-idx1-ubyte.gz to /kaggle/working/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /kaggle/working/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:43<00:00, 37729.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /kaggle/working/MNIST/raw/t10k-images-idx3-ubyte.gz to /kaggle/working/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /kaggle/working/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5529906.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /kaggle/working/MNIST/raw/t10k-labels-idx1-ubyte.gz to /kaggle/working/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_size = len(train_dataset)\n",
        "total_test_size = len(test_dataset)\n",
        "total_dev_size = len(dev_dataset)\n",
        "\n",
        "classes = 10\n",
        "input_dim = 784\n",
        "\n",
        "batch_size = 128\n",
        "epochs_per_client = 3\n",
        "learning_rate = 2e-2"
      ],
      "metadata": {
        "id": "DX0sjIoO3s3l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURE BEFORE RUNNIG\n",
        "# MAKE SURE SAME FOR BOTH SERVER AND ALL CLIENTS' CODE\n",
        "num_clients = 2\n",
        "rounds = 2"
      ],
      "metadata": {
        "id": "EJ9nYwFv33N6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURE CLIENT ID TO BE UNIQUE FOR ALL CLIENT\n",
        "# Ex. If there are 3 clients, the only possible client ids are 0, 1, 2\n",
        "# Each client will be assigned to one of those ids\n",
        "client_id = 0"
      ],
      "metadata": {
        "id": "20LYeX8hScnv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client_code = f'L{client_id+1}'"
      ],
      "metadata": {
        "id": "Vj6eu5x5SiYM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHAIN_URL = 'https://2347b804-83c2-481a-9697-c5f6de62b5ab-00-2qo920vvfssrt.spock.replit.dev/chain'\n",
        "MINE_URL = 'https://2347b804-83c2-481a-9697-c5f6de62b5ab-00-2qo920vvfssrt.spock.replit.dev/mine'\n",
        "SEND_TRNS_URL = 'https://2347b804-83c2-481a-9697-c5f6de62b5ab-00-2qo920vvfssrt.spock.replit.dev/transactions/new'\n",
        "GET_TRNS_URL = 'https://2347b804-83c2-481a-9697-c5f6de62b5ab-00-2qo920vvfssrt.spock.replit.dev/current'\n",
        "PREV_BLK_URL = 'https://2347b804-83c2-481a-9697-c5f6de62b5ab-00-2qo920vvfssrt.spock.replit.dev/last'"
      ],
      "metadata": {
        "id": "UgWcuAfe35T0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_size, total_dev_size, total_test_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXTPJc8k359T",
        "outputId": "d9c6d606-3b9c-4533-e8ee-0218873e288f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49800, 10200, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader(DataLoader):\n",
        "        def __init__(self, dl, device):\n",
        "            self.dl = dl\n",
        "            self.device = device\n",
        "\n",
        "        def __iter__(self):\n",
        "            for batch in self.dl:\n",
        "                yield to_device(batch, self.device)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.dl)\n",
        "\n",
        "device = get_device()"
      ],
      "metadata": {
        "id": "Tz8uxvM7396u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FederatedNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(1, 20, 7)\n",
        "        self.conv2 = torch.nn.Conv2d(20, 40, 7)\n",
        "        self.maxpool = torch.nn.MaxPool2d(2, 2)\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.linear = torch.nn.Linear(2560, 10)\n",
        "        self.non_linearity = torch.nn.functional.relu\n",
        "        self.track_layers = {'conv1': self.conv1, 'conv2': self.conv2, 'linear': self.linear}\n",
        "\n",
        "    def forward(self, x_batch):\n",
        "        out = self.conv1(x_batch)\n",
        "        out = self.non_linearity(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.non_linearity(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.flatten(out)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "    def get_track_layers(self):\n",
        "        return self.track_layers\n",
        "\n",
        "    def apply_parameters(self, parameters_dict):\n",
        "        with torch.no_grad():\n",
        "            for layer_name in parameters_dict:\n",
        "                self.track_layers[layer_name].weight.data *= 0\n",
        "                self.track_layers[layer_name].bias.data *= 0\n",
        "                self.track_layers[layer_name].weight.data += parameters_dict[layer_name]['weight']\n",
        "                self.track_layers[layer_name].bias.data += parameters_dict[layer_name]['bias']\n",
        "\n",
        "    def get_parameters(self):\n",
        "        parameters_dict = dict()\n",
        "        for layer_name in self.track_layers:\n",
        "            parameters_dict[layer_name] = {\n",
        "                'weight': self.track_layers[layer_name].weight.data,\n",
        "                'bias': self.track_layers[layer_name].bias.data\n",
        "            }\n",
        "        return parameters_dict\n",
        "\n",
        "    def batch_accuracy(self, outputs, labels):\n",
        "        with torch.no_grad():\n",
        "            _, predictions = torch.max(outputs, dim=1)\n",
        "            return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
        "\n",
        "    def _process_batch(self, batch):\n",
        "        images, labels = batch\n",
        "        outputs = self(images)\n",
        "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
        "        accuracy = self.batch_accuracy(outputs, labels)\n",
        "        return (loss, accuracy)\n",
        "\n",
        "    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.SGD):\n",
        "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size, shuffle=True), device)\n",
        "        optimizer = opt(self.parameters(), lr)\n",
        "        history = []\n",
        "        for epoch in range(epochs):\n",
        "            losses = []\n",
        "            accs = []\n",
        "            for batch in dataloader:\n",
        "                loss, acc = self._process_batch(batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                loss.detach()\n",
        "                losses.append(loss)\n",
        "                accs.append(acc)\n",
        "            avg_loss = torch.stack(losses).mean().item()\n",
        "            avg_acc = torch.stack(accs).mean().item()\n",
        "            history.append((avg_loss, avg_acc))\n",
        "        return history\n",
        "\n",
        "    def evaluate(self, dataset, batch_size=128):\n",
        "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n",
        "        losses = []\n",
        "        accs = []\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                loss, acc = self._process_batch(batch)\n",
        "                losses.append(loss)\n",
        "                accs.append(acc)\n",
        "        avg_loss = torch.stack(losses).mean().item()\n",
        "        avg_acc = torch.stack(accs).mean().item()\n",
        "        return (avg_loss, avg_acc)"
      ],
      "metadata": {
        "id": "I0o4uFFz3-VH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Client:\n",
        "    def __init__(self, client_id, dataset):\n",
        "        self.client_id = client_id\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def get_dataset_size(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def get_client_id(self):\n",
        "        return self.client_id\n",
        "\n",
        "    def train(self, parameters_dict):\n",
        "        net = to_device(FederatedNet(), device)\n",
        "        net.apply_parameters(parameters_dict)\n",
        "        train_history = net.fit(self.dataset, epochs_per_client, learning_rate, batch_size)\n",
        "        print('{}: Loss = {}, Accuracy = {}'.format(self.client_id, round(train_history[-1][0], 4), round(train_history[-1][1], 4)))\n",
        "        return net.get_parameters()"
      ],
      "metadata": {
        "id": "EEL4Btdb4BhH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compress_params(params):\n",
        "  compressed = pickle.dumps(params)\n",
        "  params_bytes = base64.b64encode(compressed)\n",
        "  params_bystr = params_bytes.decode('ascii')\n",
        "  return params_bystr\n",
        "\n",
        "def decompress_params(params):\n",
        "  decompressed = params.encode(\"ascii\")\n",
        "  decompressed = base64.b64decode(decompressed)\n",
        "  decompressed = pickle.loads(decompressed)\n",
        "  return decompressed"
      ],
      "metadata": {
        "id": "UJbe0ncV4D5j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples_per_client = total_train_size // num_clients\n",
        "client_datasets = random_split(train_dataset, [min(i + examples_per_client,\n",
        "           total_train_size) - i for i in range(0, total_train_size, examples_per_client)])\n",
        "clients = [Client('client_' + str(i), client_datasets[i]) for i in range(num_clients)]"
      ],
      "metadata": {
        "id": "caNXOZBD4F6L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prev_block_index = None\n",
        "for i in range(rounds):\n",
        "  response = requests.get(PREV_BLK_URL)\n",
        "  block_index = response.json()['chain']['index']\n",
        "\n",
        "  if i == 0:\n",
        "    prev_block_index = block_index\n",
        "\n",
        "  new_block = True if prev_block_index != block_index else False\n",
        "  while response.json()['chain']['transactions'][-2]['type'] != 'global' and not new_block:\n",
        "    sleep(45)\n",
        "    response = requests.get(PREV_BLK_URL)\n",
        "\n",
        "  prev_block_index = block_index\n",
        "\n",
        "  curr_global_gradients = response.json()['chain']['transactions'][-2]['gradients']\n",
        "  curr_global_gradients = decompress_params(curr_global_gradients)\n",
        "\n",
        "  client_parameters = clients[client_id].train(curr_global_gradients)\n",
        "  client_data_size = clients[client_id].get_dataset_size()\n",
        "  client_parameters_compressed = compress_params(client_parameters)\n",
        "  new_transaction = {\n",
        "      'type': 'local',\n",
        "      'trainer': client_code,\n",
        "      'gradient': client_parameters_compressed,\n",
        "      'data-size': client_data_size\n",
        "  }\n",
        "\n",
        "  # Send the transaction to the server\n",
        "  response = requests.post(SEND_TRNS_URL, json=new_transaction)\n",
        "  print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daWNgm9bOjLT",
        "outputId": "7e8e8635-5341-430c-b2e4-8d4faece22a2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "client_0: Loss = 0.2071, Accuracy = 0.9402\n",
            "{'message': 'Transaction will be added to Block 3'}\n",
            "client_0: Loss = 0.2076, Accuracy = 0.9401\n",
            "{'message': 'Transaction will be added to Block 5'}\n"
          ]
        }
      ]
    }
  ]
}